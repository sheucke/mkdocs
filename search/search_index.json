{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to my Documentation site Here I store documents to easily find information. Tutorials On the right site choose the information you want.","title":"Home"},{"location":"#welcome-to-my-documentation-site","text":"Here I store documents to easily find information.","title":"Welcome to my Documentation site"},{"location":"#tutorials","text":"On the right site choose the information you want.","title":"Tutorials"},{"location":"Conda/","text":"Conda and Bioconda Download and install conda (type: empty , yes , empty , yes ) wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh /bin/bash Miniconda3-latest-Linux-x86_64.sh source ~/.bashrc Channels and their order conda config --add channels conda-forge bioconda defaults conda config --show channels conda config --remove channels Conda basics conda search foo # search for \"foo\" in your channels conda install foo # Install \"foo\" from your channels conda remove foo # Remove \"foo\" from your channels conda update foo # Update the version of \"foo\" conda info # print the version of conda Conda environments conda env list # list environments conda create -n foo package1 package2 # create a new environment named \"foo\" with 2 packages conda env remove -n foo # Remove the \"foo\" environment conda activate foo # Activate the \"foo\" environment conda deactivate conda env export > foo.yaml # Export the environment to a YAML file conda env create -f foo.yaml -n bar # Create an environment named \"bar\" from a YAML file","title":"Conda"},{"location":"Conda/#conda-and-bioconda","text":"","title":"Conda and Bioconda"},{"location":"Conda/#download-and-install-conda-type-empty-yes-empty-yes","text":"wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh /bin/bash Miniconda3-latest-Linux-x86_64.sh source ~/.bashrc","title":"Download and install conda (type: empty, yes, empty, yes)"},{"location":"Conda/#channels-and-their-order","text":"conda config --add channels conda-forge bioconda defaults conda config --show channels conda config --remove channels","title":"Channels and their order"},{"location":"Conda/#conda-basics","text":"conda search foo # search for \"foo\" in your channels conda install foo # Install \"foo\" from your channels conda remove foo # Remove \"foo\" from your channels conda update foo # Update the version of \"foo\" conda info # print the version of conda","title":"Conda basics"},{"location":"Conda/#conda-environments","text":"conda env list # list environments conda create -n foo package1 package2 # create a new environment named \"foo\" with 2 packages conda env remove -n foo # Remove the \"foo\" environment conda activate foo # Activate the \"foo\" environment conda deactivate conda env export > foo.yaml # Export the environment to a YAML file conda env create -f foo.yaml -n bar # Create an environment named \"bar\" from a YAML file","title":"Conda environments"},{"location":"Django/","text":"Django tutorial Open folder in WSL (VSCODE) to have working server! Setup Create project with virtual environment and version control 1.Create new folder mkdir project_name cd project_name 2.Create a Readme with project description touch readme.md 3.Use Conda environment conda create -n dj conda activate dj conda install django list all packages conda list create an envirmonment file conda env export > environment.yaml 4.Create a Git repository git init git add . git commit -m \"First Commit\" git git remote add origin <repo name> git push origin master git branch develop git checkout develop 5.Create gitignore file touch .gitignore https://www.toptal.com/developers/gitignore 5.Create a Django Project django-admin startproject dj_bootcamp . 6.Save as Workspace 7.Create User for django Initialize database. python manage.py migrate python manage.py createsuperuser python manage.py runserver 8.Create a django app python manage.py startapp products 9.Create database model in apps models.py class Product(models.Model): # id = models.Autofield() ; id comes automatically from django title = models.CharField(max_length=200) content = models.TextField(null=True, blank=True) price = models.IntegerField(default=0) https://docs.djangoproject.com/en/3.1/ref/models/fields/#field-types In settings.py add the apps: # Application definition INSTALLED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'products', 'profiles', ] Then use: python manage.py makemigrations python manage.py migrate to create the tables in your db.sqlite3 database. 10.Register the models for the admin Page access In the admin.py file inside the app folder add the lines: # Register your models here. from .models import Product admin.site.register(Product) 11. Put entries in the database You can use the browser interface: Or in the terminal open a django shell: python manage.py shell from products.models import Product Product.objects.create(title='Raspberry Pi') obj = Product.objects.get(id=1) # as object qs = Product.objects.all() # as queryset list for obj in qs: print(obj.id) obj = Product.objects.get(id=2) obj.content = \"some new content\" obj.save() exit() 12. Django Views Views can be function or class based and are created inside the views.py files. from django.http import HttpResponse from django.shortcuts import render # Create your views here. # function based views def home_view(request, *args, **kwargs): return HttpResponse(\"<h1>Hello World</h1>\") # class based views class HomeView(): pass To create a url for this view go to urls.py and import the views and create a path for them. from django.contrib import admin from django.urls import path from products import views urlpatterns = [ path('search/', views.home_view), path('admin/', admin.site.urls), ] 13. Get an item from the database In the views.py add an new function product_detail_view. from django.http import HttpResponse from django.shortcuts import render from .models import Product # Create your views here. # function based views def home_view(request, *args, **kwargs): return HttpResponse(\"<h1>Hello World</h1>\") # class based views # class HomeView(): # pass def product_detail_view(request, *args, **kwargs): obj = Product.objects.get(id=1) return HttpResponse(f\"Product id {obj.id}\") Then add the url path to urls.py. from django.contrib import admin from django.urls import path from products import views urlpatterns = [ path('search/', views.home_view), path('products/1/', views.product_detail_view), path('admin/', admin.site.urls), ] To make the urls dynamic change the view function and urls path. def product_detail_view(request, pk): obj = Product.objects.get(pk=pk) return HttpResponse(f\"Product id {obj.id}\") urlpatterns = [ path('search/', views.home_view), path('products/<int:pk>/', views.product_detail_view), path('admin/', admin.site.urls), ] To handle errors with not existing database items. def product_detail_view(request, pk): try: obj = Product.objects.get(pk=pk) except Product.DoesNotExist: raise Http404 return HttpResponse(f\"Product id {obj.id}\") Different way to import views to urls.py from django.contrib import admin from django.urls import path from products.views import home_view, product_detail_view urlpatterns = [ path('search/', home_view), path('products/<int:pk>/', product_detail_view), path('admin/', admin.site.urls), ] 13. Django Templates Create a folder named templates in the root directory of the project. In the new folder create an html file called home.html. <h1>Hello, {{ name }}!</h1> In settings.py add the templates folder to the 'DIRS' list. TEMPLATES = [ { 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS': [BASE_DIR / \"templates\"], 'APP_DIRS': True, 'OPTIONS': { 'context_processors': [ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', ], }, }, ] In the views.py change the home_view. def home_view(request, *args, **kwargs): # return HttpResponse(\"<h1>Hello World</h1>\") context = {\"name\": \"Sebastian\"} return render(request, \"home.html\", context) 14. Template inheritance This is used to inherit a base.html to all other webpages of the app. Create a base.html file in the templates folder. <!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"UTF-8\" /> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" /> <title>Home Page</title> <style> h1 { color: red; } </style> </head> <body> {% block content %} {% endblock %} </body> </html> Then change your home.html. {% extends \"base.html\" %} {% block content %} <h1>Hello, {{ name }}!</h1> {% endblock %} Example for some database information. def product_detail_view(request, pk): try: obj = Product.objects.get(pk=pk) except Product.DoesNotExist: raise Http404 return render(request, \"products/detail.html\", {\"object\": obj}) {% extends \"base.html\" %} {% block content %} <h1>{{ object.title }}</h1> {% if object.content %} <p>{{ object.content }}</p> {% else %} <p>Content soon.</p> {% endif %} {% endblock %} 15. Add Data with Django Forms Have a user send data. HTTP Methods: GET, POST, PUT, DELETE CRUD: Create, Retrieve, Update, Destroy database items Every request can have HTTP Methods. def any_view(request, *args, **kwargs): print(request.POST) print(request.GET) print(request.method == \"POST\") print(request.method == \"GET\") return render(request, 'any_view.html', {}) Create a function in views.py. def product_create_view(request, *args, **kwargs): print(request.POST) print(request.GET) return render(request, \"products/forms.html\", {}) Then create the forms.html file. Dont forget the CSRF (Cross site reference forgery token). Django provides this as middleware in the settings.py. {% extends \"base.html\" %} {% block content %} <form method=\"POST\" action=\".\"> {% csrf_token %} <input type=\"text\" name=\"something\" /> <button type=\"submit\">Send data</button> </form> {% endblock %} And then register that html file in the urls.py file as a path. from products.views import home_view, product_detail_view, product_list_view, product_create_view urlpatterns = [ path('search/', home_view), path('products/<int:pk>/', product_detail_view), path('products/', product_list_view), path('products/create/', product_create_view), path('admin/', admin.site.urls), ] Create Forms the Django way. First create a forms.py file in the app. from django import forms class ProductForm(forms.Form): title = forms.CharField() Change the forms.html. {% extends \"base.html\" %} {% block content %} <form method=\"POST\" action=\".\"> {% csrf_token %} <input type=\"text\" name=\"title\" /> <button type=\"submit\">Send data</button> </form> {% endblock %} And change the funtion in the views.py. def product_create_view(request, *args, **kwargs): if request.method == \"POST\": post_data = request.POST or None if post_data != None: my_form = ProductForm(request.POST) if my_form.is_valid(): print(my_form.cleaned_data.get(\"title\")) title_from_input = my_form.cleaned_data.get(\"title\") Product.objects.create(title=title_from_input) return render(request, \"products/forms.html\", {}) And again to make it better change the following. def product_create_view(request, *args, **kwargs): form = ProductForm(request.POST or None) if form.is_valid(): data = form.cleaned_data Product.objects.create(**data) form = ProductForm() # reinitialize return render(request, \"products/forms.html\", {\"form\": form}) And then in the forms.html {% extends \"base.html\" %} {% block content %} <form method=\"POST\" action=\".\"> {% csrf_token %} {{ form.as_p }} <button type=\"submit\">Send data</button> </form> {% endblock %} And yet again another way with ModelForms. from django import forms from .models import Product class ProductForm(forms.ModelForm): class Meta: model = Product fields = [ 'title', 'content' ] def clean_title(self): data = self.cleaned_data.get('title') if len(data) < 4: raise forms.ValidationError(\"This is not long enough\") return data def product_create_view(request, *args, **kwargs): form = ProductForm(request.POST or None) if form.is_valid(): obj = form.save(commit=False) obj.save() form = ProductForm() return render(request, \"products/forms.html\", {\"form\": form}) 16. Login and Register Users In django shell you can get all the unique users by: from django.contrib.auth import get_user_model User = get_user_model() User.objects.all() User.objects.create(username='sebastian', email='sebastian@gmail.com) # or User.objects.create_user(\"sebastian\", \"sebastian@gmail.com\", \"password\") user = User.objects.get(username=\"sebastian\") user.set_password(\"123456\") user.save() Create a new app called accounts for the users. And inside the accounts a forms.py file. from django.contrib.auth import get_user_model from django import forms User = get_user_model() non_allowed_usernames = ['abc'] class RegisterForm(forms.Form): username = forms.CharField() email = forms.EmailField() password1 = forms.CharField( label='Password', widget=forms.PasswordInput( attrs={\"class\": \"form-control\", \"id\": \"user-password\"} ) ) password2 = forms.CharField( label='Confirm Password', widget=forms.PasswordInput( attrs={\"class\": \"form-control\", \"id\": \"user-confirm-password\"} ) ) def clean_username(self): username = self.cleaned_data.get(\"username\") qs = User.objects.filter(username__iexact=username) if username in non_allowed_usernames: raise forms.ValidationError( \"This is an invalid username, please pick another.\") if not qs.exists(): raise forms.ValidationError( \"This is an invalid username, please pick another.\") return username def clean_email(self): email = self.cleaned_data.get(\"email\") qs = User.objects.filter(username__iexact=email) if not qs.exists(): raise forms.ValidationError(\"This email is already in use.\") return email class LoginForm(forms.Form): username = forms.CharField() password = forms.CharField( widget=forms.PasswordInput( attrs={\"class\": \"form-control\", \"id\": \"user-password\"} ) ) def clean_username(self): username = self.cleaned_data.get(\"username\") qs = User.objects.filter(username__iexact=username) if not qs.exists(): raise forms.ValidationError(\"This is an invalid user.\") return usernam In the views.py of the accounts app. from django.contrib.auth import authenticate, login, logout, get_user_model from django.shortcuts import redirect, render from .forms import LoginForm, RegisterForm # Create your views here. User = get_user_model() def register_view(request): form = RegisterForm(request.POST or None) if form.is_valid(): username = form.cleaned_data.get(\"username\") email = form.cleaned_data.get(\"email\") password = form.cleaned_data.get(\"password1\") password2 = form.cleaned_data.get(\"password2\") try: user = User.objects.create_user(username, email, password) except: user = None if user != None: login(request, user) return redirect(\"/\") else: # attempt = request.session.get(\"attempt\") or 0 # request.session['attempt'] = attempt + 1 request.session[\"register_error\"] = 1 # True return render(request, \"forms.html\", {\"form\": form}) def login_view(request): form = LoginForm(request.POST or None) if form.is_valid(): username = form.cleaned_data.get(\"username\") password = form.cleaned_data.get(\"password\") user = authenticate(request, username=username, password=password) if user != None: login(request, user) return redirect(\"/\") else: # attempt = request.session.get(\"attempt\") or 0 # request.session['attempt'] = attempt + 1 request.session[\"invalid_user\"] = 1 # True return render(request, \"forms.html\", {\"form\": form}) def logout_view(request): logout(request) return redirect(\"/login\") Create a path in the urls.py urlpatterns = [ path('search/', home_view), path('products/<int:pk>/', product_detail_view), path('products/', product_list_view), path('products/create/', product_create_view), path('admin/', admin.site.urls), path('login/', login_view), path('logout/', logout_view), path('register/', register_view), ] And in the settings.py file add the lines. LOGIN_URL = \"/login\" LOGIN_REDIRECT_URL = \"/\" 17. Automated Tests When projects grow, automated tests (also known as unit tests) are very important to ensure that the app is working with little to no bugs. from django.conf import settings from django.contrib.auth import get_user_model from django.http import response from django.shortcuts import redirect from django.test import TestCase # Create your tests here. User = get_user_model() class UserTestCase(TestCase): def setUp(self): # Python builtin unittest user_a = User(username='sebastian', email='sebastian@gmail.com') user_a_pw = 'some_123_password' self.user_a_pw = user_a_pw user_a.is_staff = True user_a.is_superuser = True user_a.set_password(user_a_pw) user_a.save() print(user_a.id) self.user_a = user_a def test_user_exists(self): user_count = User.objects.all().count() self.assertEqual(user_count, 1) self.assertNotEqual(user_count, 0) def test_user_password(self): user_a = User.objects.get(username=\"sebastian\") self.assertTrue(self.user_a.check_password(self.user_a_pw)) def test_login_url(self): login_url = settings.LOGIN_URL # response = self.client.post(url, {}, follow=True) data = {\"username\": \"sebastian\", \"password\": \"some_123_password\"} response = self.client.post(login_url, data, follow=True) print(dir(response)) status_code = response.status_code redirect_path = response.request.get(\"PATH_INFO\") self.assertEqual(redirect_path, settings.LOGIN_URL) self.assertEqual(status_code, 200) 18. Orders and Inventory In models.py of the products app add an inventory column from django.conf import settings from django.db import models # Create your models here. User = settings.AUTH_USER_MODEL class Product(models.Model): # id = models.Autofield() ; id comes automatically from django user = models.ForeignKey(User, null=True, on_delete=models.SET_NULL) title = models.CharField(max_length=200) content = models.TextField(null=True, blank=True) price = models.DecimalField( max_digits=10, decimal_places=2, default=0.00) # 0.99 inventory = models.IntegerField(default=0) def has_inventory(self): return self.inventory > 0 # True or False And for the orders create a new app orders and add it to the seetings.py. Then create the database table columns in the models.py. from django.contrib.auth import get_user_model from django.db import models from products.models import Product # Create your models here. User = get_user_model() ORDER_STATUS_CHOICES = ( ('created', 'Created'), ('stale', 'Stale'), ('paid', 'Paid'), ('shipped', 'Shipped'), ('refunded', 'Refunded'), ) class Order(models.Model): user = models.ForeignKey(User, null=True, on_delete=models.SET_NULL) product = models.ForeignKey(Product, null=True, on_delete=models.SET_NULL) status = models.CharField( max_length=20, choices=ORDER_STATUS_CHOICES, default='created') subtotal = models.DecimalField( max_digits=10, decimal_places=2, default=0.00) tax = models.DecimalField( max_digits=10, decimal_places=2, default=0.00) total = models.DecimalField( max_digits=10, decimal_places=2, default=0.00) paid = models.DecimalField( max_digits=10, decimal_places=2, default=0.00) shipping_adress = models.TextField(blank=True, null=True) billing_adress = models.TextField(blank=True, null=True) # automatically datatime object timestamp = models.DateTimeField(auto_now_add=True)","title":"Django"},{"location":"Django/#django-tutorial","text":"Open folder in WSL (VSCODE) to have working server!","title":"Django tutorial"},{"location":"Django/#setup","text":"Create project with virtual environment and version control","title":"Setup"},{"location":"Django/#1create-new-folder","text":"mkdir project_name cd project_name","title":"1.Create new folder"},{"location":"Django/#2create-a-readme-with-project-description","text":"touch readme.md","title":"2.Create a Readme with project description"},{"location":"Django/#3use-conda-environment","text":"conda create -n dj conda activate dj conda install django list all packages conda list create an envirmonment file conda env export > environment.yaml","title":"3.Use Conda environment"},{"location":"Django/#4create-a-git-repository","text":"git init git add . git commit -m \"First Commit\" git git remote add origin <repo name> git push origin master git branch develop git checkout develop","title":"4.Create a Git repository"},{"location":"Django/#5create-gitignore-file","text":"touch .gitignore https://www.toptal.com/developers/gitignore","title":"5.Create gitignore file"},{"location":"Django/#5create-a-django-project","text":"django-admin startproject dj_bootcamp .","title":"5.Create a Django Project"},{"location":"Django/#6save-as-workspace","text":"","title":"6.Save as Workspace"},{"location":"Django/#7create-user-for-django","text":"Initialize database. python manage.py migrate python manage.py createsuperuser python manage.py runserver","title":"7.Create User for django"},{"location":"Django/#8create-a-django-app","text":"python manage.py startapp products","title":"8.Create a django app"},{"location":"Django/#9create-database-model-in-apps-modelspy","text":"class Product(models.Model): # id = models.Autofield() ; id comes automatically from django title = models.CharField(max_length=200) content = models.TextField(null=True, blank=True) price = models.IntegerField(default=0) https://docs.djangoproject.com/en/3.1/ref/models/fields/#field-types In settings.py add the apps: # Application definition INSTALLED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'products', 'profiles', ] Then use: python manage.py makemigrations python manage.py migrate to create the tables in your db.sqlite3 database.","title":"9.Create database model in apps models.py"},{"location":"Django/#10register-the-models-for-the-admin-page-access","text":"In the admin.py file inside the app folder add the lines: # Register your models here. from .models import Product admin.site.register(Product)","title":"10.Register the models for the admin Page access"},{"location":"Django/#11-put-entries-in-the-database","text":"You can use the browser interface: Or in the terminal open a django shell: python manage.py shell from products.models import Product Product.objects.create(title='Raspberry Pi') obj = Product.objects.get(id=1) # as object qs = Product.objects.all() # as queryset list for obj in qs: print(obj.id) obj = Product.objects.get(id=2) obj.content = \"some new content\" obj.save() exit()","title":"11. Put entries in the database"},{"location":"Django/#12-django-views","text":"Views can be function or class based and are created inside the views.py files. from django.http import HttpResponse from django.shortcuts import render # Create your views here. # function based views def home_view(request, *args, **kwargs): return HttpResponse(\"<h1>Hello World</h1>\") # class based views class HomeView(): pass To create a url for this view go to urls.py and import the views and create a path for them. from django.contrib import admin from django.urls import path from products import views urlpatterns = [ path('search/', views.home_view), path('admin/', admin.site.urls), ]","title":"12. Django Views"},{"location":"Django/#13-get-an-item-from-the-database","text":"In the views.py add an new function product_detail_view. from django.http import HttpResponse from django.shortcuts import render from .models import Product # Create your views here. # function based views def home_view(request, *args, **kwargs): return HttpResponse(\"<h1>Hello World</h1>\") # class based views # class HomeView(): # pass def product_detail_view(request, *args, **kwargs): obj = Product.objects.get(id=1) return HttpResponse(f\"Product id {obj.id}\") Then add the url path to urls.py. from django.contrib import admin from django.urls import path from products import views urlpatterns = [ path('search/', views.home_view), path('products/1/', views.product_detail_view), path('admin/', admin.site.urls), ] To make the urls dynamic change the view function and urls path. def product_detail_view(request, pk): obj = Product.objects.get(pk=pk) return HttpResponse(f\"Product id {obj.id}\") urlpatterns = [ path('search/', views.home_view), path('products/<int:pk>/', views.product_detail_view), path('admin/', admin.site.urls), ] To handle errors with not existing database items. def product_detail_view(request, pk): try: obj = Product.objects.get(pk=pk) except Product.DoesNotExist: raise Http404 return HttpResponse(f\"Product id {obj.id}\") Different way to import views to urls.py from django.contrib import admin from django.urls import path from products.views import home_view, product_detail_view urlpatterns = [ path('search/', home_view), path('products/<int:pk>/', product_detail_view), path('admin/', admin.site.urls), ]","title":"13. Get an item from the database"},{"location":"Django/#13-django-templates","text":"Create a folder named templates in the root directory of the project. In the new folder create an html file called home.html. <h1>Hello, {{ name }}!</h1> In settings.py add the templates folder to the 'DIRS' list. TEMPLATES = [ { 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS': [BASE_DIR / \"templates\"], 'APP_DIRS': True, 'OPTIONS': { 'context_processors': [ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', ], }, }, ] In the views.py change the home_view. def home_view(request, *args, **kwargs): # return HttpResponse(\"<h1>Hello World</h1>\") context = {\"name\": \"Sebastian\"} return render(request, \"home.html\", context)","title":"13. Django Templates"},{"location":"Django/#14-template-inheritance","text":"This is used to inherit a base.html to all other webpages of the app. Create a base.html file in the templates folder. <!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"UTF-8\" /> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" /> <title>Home Page</title> <style> h1 { color: red; } </style> </head> <body> {% block content %} {% endblock %} </body> </html> Then change your home.html. {% extends \"base.html\" %} {% block content %} <h1>Hello, {{ name }}!</h1> {% endblock %} Example for some database information. def product_detail_view(request, pk): try: obj = Product.objects.get(pk=pk) except Product.DoesNotExist: raise Http404 return render(request, \"products/detail.html\", {\"object\": obj}) {% extends \"base.html\" %} {% block content %} <h1>{{ object.title }}</h1> {% if object.content %} <p>{{ object.content }}</p> {% else %} <p>Content soon.</p> {% endif %} {% endblock %}","title":"14. Template inheritance"},{"location":"Django/#15-add-data-with-django-forms","text":"Have a user send data. HTTP Methods: GET, POST, PUT, DELETE CRUD: Create, Retrieve, Update, Destroy database items Every request can have HTTP Methods. def any_view(request, *args, **kwargs): print(request.POST) print(request.GET) print(request.method == \"POST\") print(request.method == \"GET\") return render(request, 'any_view.html', {}) Create a function in views.py. def product_create_view(request, *args, **kwargs): print(request.POST) print(request.GET) return render(request, \"products/forms.html\", {}) Then create the forms.html file. Dont forget the CSRF (Cross site reference forgery token). Django provides this as middleware in the settings.py. {% extends \"base.html\" %} {% block content %} <form method=\"POST\" action=\".\"> {% csrf_token %} <input type=\"text\" name=\"something\" /> <button type=\"submit\">Send data</button> </form> {% endblock %} And then register that html file in the urls.py file as a path. from products.views import home_view, product_detail_view, product_list_view, product_create_view urlpatterns = [ path('search/', home_view), path('products/<int:pk>/', product_detail_view), path('products/', product_list_view), path('products/create/', product_create_view), path('admin/', admin.site.urls), ] Create Forms the Django way. First create a forms.py file in the app. from django import forms class ProductForm(forms.Form): title = forms.CharField() Change the forms.html. {% extends \"base.html\" %} {% block content %} <form method=\"POST\" action=\".\"> {% csrf_token %} <input type=\"text\" name=\"title\" /> <button type=\"submit\">Send data</button> </form> {% endblock %} And change the funtion in the views.py. def product_create_view(request, *args, **kwargs): if request.method == \"POST\": post_data = request.POST or None if post_data != None: my_form = ProductForm(request.POST) if my_form.is_valid(): print(my_form.cleaned_data.get(\"title\")) title_from_input = my_form.cleaned_data.get(\"title\") Product.objects.create(title=title_from_input) return render(request, \"products/forms.html\", {}) And again to make it better change the following. def product_create_view(request, *args, **kwargs): form = ProductForm(request.POST or None) if form.is_valid(): data = form.cleaned_data Product.objects.create(**data) form = ProductForm() # reinitialize return render(request, \"products/forms.html\", {\"form\": form}) And then in the forms.html {% extends \"base.html\" %} {% block content %} <form method=\"POST\" action=\".\"> {% csrf_token %} {{ form.as_p }} <button type=\"submit\">Send data</button> </form> {% endblock %} And yet again another way with ModelForms. from django import forms from .models import Product class ProductForm(forms.ModelForm): class Meta: model = Product fields = [ 'title', 'content' ] def clean_title(self): data = self.cleaned_data.get('title') if len(data) < 4: raise forms.ValidationError(\"This is not long enough\") return data def product_create_view(request, *args, **kwargs): form = ProductForm(request.POST or None) if form.is_valid(): obj = form.save(commit=False) obj.save() form = ProductForm() return render(request, \"products/forms.html\", {\"form\": form})","title":"15. Add Data with Django Forms"},{"location":"Django/#16-login-and-register-users","text":"In django shell you can get all the unique users by: from django.contrib.auth import get_user_model User = get_user_model() User.objects.all() User.objects.create(username='sebastian', email='sebastian@gmail.com) # or User.objects.create_user(\"sebastian\", \"sebastian@gmail.com\", \"password\") user = User.objects.get(username=\"sebastian\") user.set_password(\"123456\") user.save() Create a new app called accounts for the users. And inside the accounts a forms.py file. from django.contrib.auth import get_user_model from django import forms User = get_user_model() non_allowed_usernames = ['abc'] class RegisterForm(forms.Form): username = forms.CharField() email = forms.EmailField() password1 = forms.CharField( label='Password', widget=forms.PasswordInput( attrs={\"class\": \"form-control\", \"id\": \"user-password\"} ) ) password2 = forms.CharField( label='Confirm Password', widget=forms.PasswordInput( attrs={\"class\": \"form-control\", \"id\": \"user-confirm-password\"} ) ) def clean_username(self): username = self.cleaned_data.get(\"username\") qs = User.objects.filter(username__iexact=username) if username in non_allowed_usernames: raise forms.ValidationError( \"This is an invalid username, please pick another.\") if not qs.exists(): raise forms.ValidationError( \"This is an invalid username, please pick another.\") return username def clean_email(self): email = self.cleaned_data.get(\"email\") qs = User.objects.filter(username__iexact=email) if not qs.exists(): raise forms.ValidationError(\"This email is already in use.\") return email class LoginForm(forms.Form): username = forms.CharField() password = forms.CharField( widget=forms.PasswordInput( attrs={\"class\": \"form-control\", \"id\": \"user-password\"} ) ) def clean_username(self): username = self.cleaned_data.get(\"username\") qs = User.objects.filter(username__iexact=username) if not qs.exists(): raise forms.ValidationError(\"This is an invalid user.\") return usernam In the views.py of the accounts app. from django.contrib.auth import authenticate, login, logout, get_user_model from django.shortcuts import redirect, render from .forms import LoginForm, RegisterForm # Create your views here. User = get_user_model() def register_view(request): form = RegisterForm(request.POST or None) if form.is_valid(): username = form.cleaned_data.get(\"username\") email = form.cleaned_data.get(\"email\") password = form.cleaned_data.get(\"password1\") password2 = form.cleaned_data.get(\"password2\") try: user = User.objects.create_user(username, email, password) except: user = None if user != None: login(request, user) return redirect(\"/\") else: # attempt = request.session.get(\"attempt\") or 0 # request.session['attempt'] = attempt + 1 request.session[\"register_error\"] = 1 # True return render(request, \"forms.html\", {\"form\": form}) def login_view(request): form = LoginForm(request.POST or None) if form.is_valid(): username = form.cleaned_data.get(\"username\") password = form.cleaned_data.get(\"password\") user = authenticate(request, username=username, password=password) if user != None: login(request, user) return redirect(\"/\") else: # attempt = request.session.get(\"attempt\") or 0 # request.session['attempt'] = attempt + 1 request.session[\"invalid_user\"] = 1 # True return render(request, \"forms.html\", {\"form\": form}) def logout_view(request): logout(request) return redirect(\"/login\") Create a path in the urls.py urlpatterns = [ path('search/', home_view), path('products/<int:pk>/', product_detail_view), path('products/', product_list_view), path('products/create/', product_create_view), path('admin/', admin.site.urls), path('login/', login_view), path('logout/', logout_view), path('register/', register_view), ] And in the settings.py file add the lines. LOGIN_URL = \"/login\" LOGIN_REDIRECT_URL = \"/\"","title":"16. Login and Register Users"},{"location":"Django/#17-automated-tests","text":"When projects grow, automated tests (also known as unit tests) are very important to ensure that the app is working with little to no bugs. from django.conf import settings from django.contrib.auth import get_user_model from django.http import response from django.shortcuts import redirect from django.test import TestCase # Create your tests here. User = get_user_model() class UserTestCase(TestCase): def setUp(self): # Python builtin unittest user_a = User(username='sebastian', email='sebastian@gmail.com') user_a_pw = 'some_123_password' self.user_a_pw = user_a_pw user_a.is_staff = True user_a.is_superuser = True user_a.set_password(user_a_pw) user_a.save() print(user_a.id) self.user_a = user_a def test_user_exists(self): user_count = User.objects.all().count() self.assertEqual(user_count, 1) self.assertNotEqual(user_count, 0) def test_user_password(self): user_a = User.objects.get(username=\"sebastian\") self.assertTrue(self.user_a.check_password(self.user_a_pw)) def test_login_url(self): login_url = settings.LOGIN_URL # response = self.client.post(url, {}, follow=True) data = {\"username\": \"sebastian\", \"password\": \"some_123_password\"} response = self.client.post(login_url, data, follow=True) print(dir(response)) status_code = response.status_code redirect_path = response.request.get(\"PATH_INFO\") self.assertEqual(redirect_path, settings.LOGIN_URL) self.assertEqual(status_code, 200)","title":"17. Automated Tests"},{"location":"Django/#18-orders-and-inventory","text":"In models.py of the products app add an inventory column from django.conf import settings from django.db import models # Create your models here. User = settings.AUTH_USER_MODEL class Product(models.Model): # id = models.Autofield() ; id comes automatically from django user = models.ForeignKey(User, null=True, on_delete=models.SET_NULL) title = models.CharField(max_length=200) content = models.TextField(null=True, blank=True) price = models.DecimalField( max_digits=10, decimal_places=2, default=0.00) # 0.99 inventory = models.IntegerField(default=0) def has_inventory(self): return self.inventory > 0 # True or False And for the orders create a new app orders and add it to the seetings.py. Then create the database table columns in the models.py. from django.contrib.auth import get_user_model from django.db import models from products.models import Product # Create your models here. User = get_user_model() ORDER_STATUS_CHOICES = ( ('created', 'Created'), ('stale', 'Stale'), ('paid', 'Paid'), ('shipped', 'Shipped'), ('refunded', 'Refunded'), ) class Order(models.Model): user = models.ForeignKey(User, null=True, on_delete=models.SET_NULL) product = models.ForeignKey(Product, null=True, on_delete=models.SET_NULL) status = models.CharField( max_length=20, choices=ORDER_STATUS_CHOICES, default='created') subtotal = models.DecimalField( max_digits=10, decimal_places=2, default=0.00) tax = models.DecimalField( max_digits=10, decimal_places=2, default=0.00) total = models.DecimalField( max_digits=10, decimal_places=2, default=0.00) paid = models.DecimalField( max_digits=10, decimal_places=2, default=0.00) shipping_adress = models.TextField(blank=True, null=True) billing_adress = models.TextField(blank=True, null=True) # automatically datatime object timestamp = models.DateTimeField(auto_now_add=True)","title":"18. Orders and Inventory"},{"location":"Docker/","text":"Using docker in research Installing docker For Ubuntu follow instructions: https://docs.docker.com/engine/install/ubuntu/ In this repo there is a script for Ubuntu install, but I suggest you use official installation guidelines. There are also some postinstall steps to make it nicer to run (for example, running without sudo) For Windows, there is \"Docker Desktop\" instalation of which is explained here: https://docs.docker.com/docker-for-windows/install/ The docker is best used from a linux system and I will assume that you have one. Why use docker? The idea of docker containers may seem unintuitive to researchers or scientists. What is the point of all these additional steps, starting programs through some strange commands when all that it does is quite similar to a normal computer setup? But that is the whole point -- you wrap up your whole setup in a nice, thin way so that you can use it but so can other people. Also you from the future -- where things change, will not have to worry that your old programs do not work and you cannot access your precious data because you have prepared yourself having nice, documented and ready to use \"system in a file\". There are alternative approaches to storing such \"virtual machines\" most known is a Virtual Box, but there are many pros of using Docker for scientific computing such as: having better control of computer resources being more lightweight than VirtualBox having better tools to run multiple containers and have shared resources or shared information being able to store computer \"recipies\" (dockerfiles) instead of whole disks sharing sites such as docker hub scaling the system if needed (through docker compose, kubernetes) Let's first start by just accessing a ready made system, for example one that does have python and scientific toolbox Basic container startup Let's open simple jupyter container docker run -p 8888:8888 jupyter/minimal-notebook and manually open the link in the browser. Regardless of your native setup, you now have access to a whole toolbox with quite new versions of tools -- in this case a docker with jupyter installed, that you can for example use for some demo. That is one of basic uses of the docker and contaiers. You do not need to install whole bunch of software separately, you just need to find a docker image fitting your needs, which is quite possible. Finding usefull docker images To do that you can either use google by just putting your-search-phase docker container or by searching on docker-hub https://hub.docker.com/ Docker hub is a place where people can put their images that is, from our perspective, computer templates that can be used to create docker containers -- which are particular expressions of the template. Let's use a bit more advanced container image - a datascience one https://hub.docker.com/r/jupyter/datascience-notebook Basic docker uses The most basic docker use is to use the docker run command. The command itself actually does more than just runs a particular image, as you saw in previous examples it also searches the image if not foud locally and compiles the containers from layers. The idea of layers is a fun one -- actually if you have many variations of the same container and the only thing that will have to be downloaded or compiled will be the layers that are different between the images. This is a huge storage bonus from that -- compared with Virtual Machines. docker run -p 8888:8888 jupyter/datascience-notebook Observe that running this command we get \"already exists\" information -- previous image also used same layers Other usefull commands: docker run -v docker container rm docker container logs You can use some ready made containers as a programs, with all dependencies solved for you.There is a whole set of, for example bioinformatics images BioContainers You can for example, run a blastp tool docker run biocontainers/blast:2.2.31 blastp -help Or you may want to check this new Python 3.9 that all are talking about docker run -v $pwd/my_research:/my_research -it python:3.9.0rc1-buster or run something using this newest and latest docker run --rm -v $(pwd)/my_research:/my_research python:3.9.0rc1-buster python /my_research/new_python_example.py It may be fun and useful to run an interactive session in the container, where you access bash docker run --name python_explorations -it python:3.9.0rc1-buster bash We can also somewhat hack access to a graphical-user-interface apps, for example lets run: xhost +local:root docker run -v /tmp/.X11-unix:/tmp/.X11-unix -e DISPLAY=unix$DISPLAY ubuntu (and xhost -local:root afterwards) inside lets install geany apt-get update && apt-get install geany and start geany If you are interested in this kind of stuff (for example to share your GUI app to everyone) I recommend using X11docker which does all the setup for you Managing docker containers We can find the container you created by putting docker container list If you have never used docker before you will find there only one container, with a strange name. This is because normally containers will be created with a random name. docker container start it will start the container with previously used flags, but Making and building images docker build . docker build . --tag docker comit mycontainer myimagename Pushing to docker hub docker login --username=yourhubusername --email=youremail@company.com docker images docker tag image_id yourhubusername/verse_gapminder:firsttry docker push yourhubusername/verse_gapminder Docker tutorial # verify installation docker --version docker --help Images, Containers, Ports Image is a template for creating an environment of your choice, it's also a snapshot. It contains everything need to run an App. A container is a running instance of an Image. # pull an image from docker hub docker pull nginx # list all images docker images # run a container docker run ngnix:latest # run container in detached mode docker run -d nginx:latest # stop a container docker stop <ID> # list runing containers docker container ls docker ps # list runing and not runing containers docker ps -a # stop and remove all containers docker rm <ID> docker rm $(docker ps -aq) # quiet mode only display ID's docker rm -f $(docker ps -aq) # forcefully remove also runing containers docker container prune # prefered way of stoping and removing containers # name containers docker run --name website -d -p 3000:80 nginx:latest # format docker ps output docker ps --format=\"ID\\t{{.ID}}\\nNAME\\t{{.Names}}\\nImage\\t{{.Image}}\\nPORTS\\t{{.Ports}}\\nCOMMAND\\t{{.Command}}\\nCREATED\\t{{.CreatedAt}}\\nSTATUS\\t{{.Status}}\\n\" export FORMAT=\"ID\\t{{.ID}}\\nNAME\\t{{.Names}}\\nImage\\t{{.Image}}\\nPORTS\\t{{.Ports}}\\nCOMMAND\\t{{.Command}}\\nCREATED\\t{{.CreatedAt}}\\nSTATUS\\t{{.Status}}\\n\" docker ps --format=$FORMAT # expose ports docker run -d -p 8080:80 nginx:latest # expose multiple ports docker run -d -p 8080:80 -p 3000:80 nginx:latest Volumes Allows sharing of data (Files and Folders) between Host and between containers. # read only folder for static web content docker run --name website -v /some/content:/usr/share/nginx/html:ro -d nginx docker run --name website -v $(pwd):/usr/share/nginx/html:ro -d nginx # uses current directory content docker run --name website -v $(pwd):/usr/share/nginx/html -d nginx # removed read only flag # access a runing container via terminal docker exec -it website bash # share volume between containers docker run --name website-copy --volumes-from website -d - p 8081:80 nginx Building Images Create own images with a Dockerfile. FROM nginx:latest ADD . /usr/share/nginx/html Build image from directory with Dockerfile. docker build --tag website:latest . .dockerignore Create a .dockerignore file to specify files and folders not needed inside the container. node_modules Dockerfile .git *.fastq folder/* Reducing image size Use Alpine images. docker pull node:lts-alpine docker pull nginx:alpine Tags and versioning Allows you to control image version. Avoids breaking changes. FROM node:8-alpine docker build -t website:latest . docker tag website:latest website:version1 Docker Registries Highly scalable server side application that stores and lets you distribute Docker images. Used in CD/CI Pipeline. Run your applictions. # upload an image to docker hub docker tag website:latest sheucke/website:1 docker login docker push sheucke/website:1 # download an image docker pull sheucke/website:1 Debugging Containers # information of container in json format docker inspect <ID> # show logs docker logs <ID> docker logs -f <ID> # realtime follow # get into a container docker exec -it <ID> /bin/bash Docker Compose Anatomy of Dockerfile Set the base image to Ubuntu must be first instruction - use docker search to find images FROM ubuntu # <image> FROM ubuntu:latest # - <image>:<tag> FROM ubuntu:precise (LTS) Set the maintainer info MAINTAINER Sebastian Heucke, sebastian.heucke@med.uni-muenchen.de # <name> Copy a local/remote file into the container Also supports auto extracting of .tar.gz into destination and can copy from remote locations (ie raw.github) ADD ./index.html /var/www/index.html # <src> <dest> Prefer copy over add: http://docs.docker.com/articles/dockerfile_best-practices/#add-or-copy Copy only supports copying local files COPY ./index.html /var/www/index.html COPY . . # copy everything from the local directory to the container Sets the user name to use USER username ENV set env vars ENV KEY 1234 # <key> <value> Runs a command on the image and commits the result RUN apt-get install vim # <command> - equivalent to docker run ubuntu apt-get install vim && docker commit XXXX The WORKDIR directive is used to set where the command defined with CMD is to be executed. WORKDIR /home/dev Like an exec, preferred (A minimal $PATH only) ENTRYPOINT [\"executable\", \"param1\", \"param2\"] Execute as a shell (Your $PATH is setup this way) ENTRYPOINT command param1 param2 this: CMD [\"-l\",\"-\"] ENTRYPOINT [\"/usr/bin/wc\"] and this: ENTRYPOINT wc -l - and this: ENTRYPOINT [\"wc\", \"-l\", \"-\"] are all equivalent Exposes this port on the container to the outside world EXPOSE 80 # <port> [<port>...] Adds one or more new volumes to any container created from the image VOLUME [\"/data\"] # [<volumes>...], puts /data -> /var/lib/docker/volumes/","title":"Docker"},{"location":"Docker/#using-docker-in-research","text":"","title":"Using docker in research"},{"location":"Docker/#installing-docker","text":"For Ubuntu follow instructions: https://docs.docker.com/engine/install/ubuntu/ In this repo there is a script for Ubuntu install, but I suggest you use official installation guidelines. There are also some postinstall steps to make it nicer to run (for example, running without sudo) For Windows, there is \"Docker Desktop\" instalation of which is explained here: https://docs.docker.com/docker-for-windows/install/ The docker is best used from a linux system and I will assume that you have one.","title":"Installing docker"},{"location":"Docker/#why-use-docker","text":"The idea of docker containers may seem unintuitive to researchers or scientists. What is the point of all these additional steps, starting programs through some strange commands when all that it does is quite similar to a normal computer setup? But that is the whole point -- you wrap up your whole setup in a nice, thin way so that you can use it but so can other people. Also you from the future -- where things change, will not have to worry that your old programs do not work and you cannot access your precious data because you have prepared yourself having nice, documented and ready to use \"system in a file\". There are alternative approaches to storing such \"virtual machines\" most known is a Virtual Box, but there are many pros of using Docker for scientific computing such as: having better control of computer resources being more lightweight than VirtualBox having better tools to run multiple containers and have shared resources or shared information being able to store computer \"recipies\" (dockerfiles) instead of whole disks sharing sites such as docker hub scaling the system if needed (through docker compose, kubernetes) Let's first start by just accessing a ready made system, for example one that does have python and scientific toolbox","title":"Why use docker?"},{"location":"Docker/#basic-container-startup","text":"Let's open simple jupyter container docker run -p 8888:8888 jupyter/minimal-notebook and manually open the link in the browser. Regardless of your native setup, you now have access to a whole toolbox with quite new versions of tools -- in this case a docker with jupyter installed, that you can for example use for some demo. That is one of basic uses of the docker and contaiers. You do not need to install whole bunch of software separately, you just need to find a docker image fitting your needs, which is quite possible.","title":"Basic container startup"},{"location":"Docker/#finding-usefull-docker-images","text":"To do that you can either use google by just putting your-search-phase docker container or by searching on docker-hub https://hub.docker.com/ Docker hub is a place where people can put their images that is, from our perspective, computer templates that can be used to create docker containers -- which are particular expressions of the template. Let's use a bit more advanced container image - a datascience one https://hub.docker.com/r/jupyter/datascience-notebook","title":"Finding usefull docker images"},{"location":"Docker/#basic-docker-uses","text":"The most basic docker use is to use the docker run command. The command itself actually does more than just runs a particular image, as you saw in previous examples it also searches the image if not foud locally and compiles the containers from layers. The idea of layers is a fun one -- actually if you have many variations of the same container and the only thing that will have to be downloaded or compiled will be the layers that are different between the images. This is a huge storage bonus from that -- compared with Virtual Machines. docker run -p 8888:8888 jupyter/datascience-notebook Observe that running this command we get \"already exists\" information -- previous image also used same layers Other usefull commands: docker run -v docker container rm docker container logs You can use some ready made containers as a programs, with all dependencies solved for you.There is a whole set of, for example bioinformatics images BioContainers You can for example, run a blastp tool docker run biocontainers/blast:2.2.31 blastp -help Or you may want to check this new Python 3.9 that all are talking about docker run -v $pwd/my_research:/my_research -it python:3.9.0rc1-buster or run something using this newest and latest docker run --rm -v $(pwd)/my_research:/my_research python:3.9.0rc1-buster python /my_research/new_python_example.py It may be fun and useful to run an interactive session in the container, where you access bash docker run --name python_explorations -it python:3.9.0rc1-buster bash We can also somewhat hack access to a graphical-user-interface apps, for example lets run: xhost +local:root docker run -v /tmp/.X11-unix:/tmp/.X11-unix -e DISPLAY=unix$DISPLAY ubuntu (and xhost -local:root afterwards) inside lets install geany apt-get update && apt-get install geany and start geany If you are interested in this kind of stuff (for example to share your GUI app to everyone) I recommend using X11docker which does all the setup for you","title":"Basic docker uses"},{"location":"Docker/#managing-docker-containers","text":"We can find the container you created by putting docker container list If you have never used docker before you will find there only one container, with a strange name. This is because normally containers will be created with a random name. docker container start it will start the container with previously used flags, but","title":"Managing docker containers"},{"location":"Docker/#making-and-building-images","text":"docker build . docker build . --tag docker comit mycontainer myimagename","title":"Making and building images"},{"location":"Docker/#pushing-to-docker-hub","text":"docker login --username=yourhubusername --email=youremail@company.com docker images docker tag image_id yourhubusername/verse_gapminder:firsttry docker push yourhubusername/verse_gapminder","title":"Pushing to docker hub"},{"location":"Docker/#docker-tutorial","text":"# verify installation docker --version docker --help","title":"Docker tutorial"},{"location":"Docker/#images-containers-ports","text":"Image is a template for creating an environment of your choice, it's also a snapshot. It contains everything need to run an App. A container is a running instance of an Image. # pull an image from docker hub docker pull nginx # list all images docker images # run a container docker run ngnix:latest # run container in detached mode docker run -d nginx:latest # stop a container docker stop <ID> # list runing containers docker container ls docker ps # list runing and not runing containers docker ps -a # stop and remove all containers docker rm <ID> docker rm $(docker ps -aq) # quiet mode only display ID's docker rm -f $(docker ps -aq) # forcefully remove also runing containers docker container prune # prefered way of stoping and removing containers # name containers docker run --name website -d -p 3000:80 nginx:latest # format docker ps output docker ps --format=\"ID\\t{{.ID}}\\nNAME\\t{{.Names}}\\nImage\\t{{.Image}}\\nPORTS\\t{{.Ports}}\\nCOMMAND\\t{{.Command}}\\nCREATED\\t{{.CreatedAt}}\\nSTATUS\\t{{.Status}}\\n\" export FORMAT=\"ID\\t{{.ID}}\\nNAME\\t{{.Names}}\\nImage\\t{{.Image}}\\nPORTS\\t{{.Ports}}\\nCOMMAND\\t{{.Command}}\\nCREATED\\t{{.CreatedAt}}\\nSTATUS\\t{{.Status}}\\n\" docker ps --format=$FORMAT # expose ports docker run -d -p 8080:80 nginx:latest # expose multiple ports docker run -d -p 8080:80 -p 3000:80 nginx:latest","title":"Images, Containers, Ports"},{"location":"Docker/#volumes","text":"Allows sharing of data (Files and Folders) between Host and between containers. # read only folder for static web content docker run --name website -v /some/content:/usr/share/nginx/html:ro -d nginx docker run --name website -v $(pwd):/usr/share/nginx/html:ro -d nginx # uses current directory content docker run --name website -v $(pwd):/usr/share/nginx/html -d nginx # removed read only flag # access a runing container via terminal docker exec -it website bash # share volume between containers docker run --name website-copy --volumes-from website -d - p 8081:80 nginx","title":"Volumes"},{"location":"Docker/#building-images","text":"Create own images with a Dockerfile. FROM nginx:latest ADD . /usr/share/nginx/html Build image from directory with Dockerfile. docker build --tag website:latest .","title":"Building Images"},{"location":"Docker/#dockerignore","text":"Create a .dockerignore file to specify files and folders not needed inside the container. node_modules Dockerfile .git *.fastq folder/*","title":".dockerignore"},{"location":"Docker/#reducing-image-size","text":"Use Alpine images. docker pull node:lts-alpine docker pull nginx:alpine","title":"Reducing image size"},{"location":"Docker/#tags-and-versioning","text":"Allows you to control image version. Avoids breaking changes. FROM node:8-alpine docker build -t website:latest . docker tag website:latest website:version1","title":"Tags and versioning"},{"location":"Docker/#docker-registries","text":"Highly scalable server side application that stores and lets you distribute Docker images. Used in CD/CI Pipeline. Run your applictions. # upload an image to docker hub docker tag website:latest sheucke/website:1 docker login docker push sheucke/website:1 # download an image docker pull sheucke/website:1","title":"Docker Registries"},{"location":"Docker/#debugging-containers","text":"# information of container in json format docker inspect <ID> # show logs docker logs <ID> docker logs -f <ID> # realtime follow # get into a container docker exec -it <ID> /bin/bash","title":"Debugging Containers"},{"location":"Docker/#docker-compose","text":"","title":"Docker Compose"},{"location":"Docker/#anatomy-of-dockerfile","text":"Set the base image to Ubuntu must be first instruction - use docker search to find images FROM ubuntu # <image> FROM ubuntu:latest # - <image>:<tag> FROM ubuntu:precise (LTS) Set the maintainer info MAINTAINER Sebastian Heucke, sebastian.heucke@med.uni-muenchen.de # <name> Copy a local/remote file into the container Also supports auto extracting of .tar.gz into destination and can copy from remote locations (ie raw.github) ADD ./index.html /var/www/index.html # <src> <dest> Prefer copy over add: http://docs.docker.com/articles/dockerfile_best-practices/#add-or-copy Copy only supports copying local files COPY ./index.html /var/www/index.html COPY . . # copy everything from the local directory to the container Sets the user name to use USER username ENV set env vars ENV KEY 1234 # <key> <value> Runs a command on the image and commits the result RUN apt-get install vim # <command> - equivalent to docker run ubuntu apt-get install vim && docker commit XXXX The WORKDIR directive is used to set where the command defined with CMD is to be executed. WORKDIR /home/dev Like an exec, preferred (A minimal $PATH only) ENTRYPOINT [\"executable\", \"param1\", \"param2\"] Execute as a shell (Your $PATH is setup this way) ENTRYPOINT command param1 param2 this: CMD [\"-l\",\"-\"] ENTRYPOINT [\"/usr/bin/wc\"] and this: ENTRYPOINT wc -l - and this: ENTRYPOINT [\"wc\", \"-l\", \"-\"] are all equivalent Exposes this port on the container to the outside world EXPOSE 80 # <port> [<port>...] Adds one or more new volumes to any container created from the image VOLUME [\"/data\"] # [<volumes>...], puts /data -> /var/lib/docker/volumes/","title":"Anatomy of Dockerfile"},{"location":"HRD_score/","text":"HRD Score Task: Determine HRD Score Solution: scarHRD package ( https://github.com/sztup/scarHRD#running-on-mouse-genomes ) Description: scarHRD is an R package which determines the levels of homologous recombination deficiency (telomeric allelic imbalance, loss off heterozygosity, number of large-scale transitions) based on NGS (WES, WGS) data. The first genomic scar based homologous recombination deficiency measures were produced using SNP arrays. Since this technology has been largely replaced by next generation sequencing it has become important to develop algorithms that derive the same type of genomic scar-scores from next generation sequencing (WXS, WGS) data. In order to perform this analysis, here we introduce the scarHRD R package and show that using this method the SNP-array based and next generation sequencing based derivation of HRD scores show good correlation. Workflow overview A typical workflow of determining the genomic scar scores for a tumor sample has the following steps: Call allele specific copy number profile on paired normal-tumor BAM files. This step has to be executed before running scarHRD. We recommend using Sequenza (Favero et al. 2015) http://www.cbs.dtu.dk/biotools/sequenza/ for copy number segmentation, Other tools (e.g. ASCAT (Van Loo et al. 2010)) may also be used in this step. This step is time-consuming and compute-intensive. Determine the scar scores with scarHRD R package. This step only takes a few minutes. Sequenza Preprocessing of input files In order to obtain precise mutational and aberration patterns in a tumor sample, Sequenza requires a matched normal sample from the same patient. Typically, the following files are needed to get started with Sequenza: A BAM file (or a derived pileup file) from the tumor specimen. A BAM file (or a derived pileup file) from the normal specimen. A FASTA reference genomic sequence file The normal and tumor BAM files are processed together to generate a seqz file, which is the required input for the analysis. It is possible to generate a seqz starting from other processed data, such as pileup, or VCF files. The available options are described in the sequenza-utils manual pages. The sequenza-utils command provides various tools; here we highlight only the basic usage: Process a FASTA file to produce a GC Wiggle track file (has to be done only once): sequenza\u2212utils gc_wiggle \u2212w 50 --fasta hg19.fa -o hg19.gc50Base.wig.gz Process BAM and Wiggle files to produce a seqz file: sequenza\u2212utils bam2seqz -n normal.bam -t tumor.bam --fasta hg19.fa \\ -gc hg19.gc50Base.wig.gz -o out.seqz.gz Post-process by binning the original seqz file: sequenza\u2212utils seqz_binning --seqz out.seqz.gz -w 50 -o out small.seqz.gz The small.seqz.gz would be the input to scarHRD . scarHRD The scarHRD input may be a detailed segmentation file from Sequenza, in case there is a reliable estimation of ploidy of the tumor sample is known, it should be sumbitted in the ploidy argument of the scarHRD function, otherwise ploidy between 1 and 5.5 will be tested: a<-read.table(\"/examples/test1.small.seqz.gz\", header=T) head(a) ## chromosome position base.ref depth.normal depth.tumor depth.ratio Af ## 1 chr1 12975 N 7 20 2.841 1.000 ## 2 chr1 13020 A 8 28 3.500 0.964 ## 3 chr1 13026 N 15 43 2.964 1.000 ## 4 chr1 13038 T 11 35 3.182 0.971 ## 5 chr1 13041 A 11 37 3.364 0.946 ## 6 chr1 13077 N 26 65 2.465 1.000 ## Bf zygosity.normal GC.percent good.reads AB.normal AB.tumor tumor.strand ## 1 0 hom 60 51 N . 0 ## 2 0 hom 60 28 A G0.036 G1.0 ## 3 0 hom 59 51 N . 0 ## 4 0 hom 59 35 T C0.029 C1.0 ## 5 0 hom 59 37 A G0.054 G0.5 ## 6 0 hom 62 51 N . 0 Usage example: library(\"scarHRD\") scar_score(\"F:/Documents/scarHRD/examples/test1.small.seqz.gz\",reference = \"grch38\", seqz=TRUE) ## Preprocessing started... ## Processing chr1: 18 variant calls; 6290 heterozygous positions; 549112 homozygous positions. ## Processing chr2: 22 variant calls; 4934 heterozygous positions; 394216 homozygous positions. ## |=================================================================| 100% ## Preprocessing finished ## Determining HRD-LOH, LST, TAI ## HRD Telomeric AI LST HRD-sum ## [1,] 1 2 0 3 scar_score(\"F:/Documents/scarHRD/examples/test2.txt\",reference = \"grch38\", seqz=FALSE) ## Determining HRD-LOH, LST, TAI ## HRD Telomeric AI LST HRD-sum ## [1,] 25 35 33 93 Parameters seg -- input file name reference -- the reference genome used, grch38 or grch37 or mouse (default: grch38 ) seqz -- TRUE if the input file is a smallo.seqz.gz file, otherwise FALSE (default: TRUE ) ploidy -- optional, previously estimated ploidy of the sample outputdir -- optional, the path to the output directory chr.in.names -- optional, default: TRUE, set to FALSE if input file does not contain 'chr' in chromosome names. Genomic scar scores Loss of Heterozygosity (HRD-LOH) The HRD-LOH score was described based on investigation in SNP-array-based copy number profiles of ovarian cancer (Abkevich et al. 2012). In this paper the authors showed that the samples with deficient BRCA1, BRCA2 have higher HRD-LOH scores compared to BRCA-intact samples, thus this measurement may be a reliable tool to estimate the sample's homologous recombination capacity. The definition of a sample's HRD-LOH score is the number of 15 Mb exceeding LOH regions which do not cover the whole chromosome. In the first paper publishing HRD-LOH-score (Abkevich et al., 2012) the authors examine the correlation between HRD-LOH-score and HR deficiency calculated for different LOH region length cut-offs. In that paper the cut-off of 15 Mb approximately in the middle of the interval was arbitrarily selected for further analysis. The authors argue that the rational for this selection rather than selecting the cut-off with the lowest p-value is that the latter cut-off is more sensitive to statistical noise present in the data. In our manuscript we also investigated if this 15 Mb cutoff is appropriate for WXS-based HRD-LOH score.We followed the same principles as Abkievits et al, thus while there was small difference between the p-values for the different minimum length cutoff values, we chose to use the same, 15 Mb limit as Abkevich et al. We also performed Spearman rank correlation between the SNP-array-based and WXS-based HRD-LOH scores for the different cutoff minimum LOH length cutoff (manuscript, Supplementary Figure S3C). Here the 14 Mb and 15 Mb cutoff-based WXS-HRD-LOH score had the highest correlation with the SNP-based HRD score. (0.700 and 0.695 respectively). This result reassured our choice of using the 15 Mb cutoff like in the SNP-array-based HRD-LOH score. Figure 1.A Visual representation of the HRD-LOH score on short theoretical chromosomes. Figure 1.B: Calculating HRD-LOH from a biallelic copy-number profile; LOH regions a, and c, would both increase the score by 1, while neither b, or d, would add to its value (b, does not pass the length requirement, and d covers a whole chromosome) Large Scale Transitions (LST) The presence of Large Scale Transitions in connection with homologous recombination deficiency was first studied in basal-like breast cancer (Popova et al. 2012). Based on SNP-array derived copy number profiles BRCA1-inactivated cases had showed higher number of large scale transitions. A large scale transition is defined as a chromosomal break between adjacent regions of at least 10 Mb, with a distance between them not larger than 3Mb. Figure 2.A: Visual representation of the LST score on short theoretical chromosomes. Figure 2.B: Calculating LST scores from a biallelic copy-number profile; events that are marked with green \"marked\" signs would increase the score, while events marked with red crosses would not. The grey areas represent the centromeric regions. (From left to right; Chromosome 1: the first event passes the definition of an LST, the second bounded by a shorter than 10 Mb segment from the right, the third is bounded by a segment from the left, which extends to the centromere, the fourth\u2019s gap is greater than 3 Mb. Chromosome 2: The first event is a valid LST, the second and third are not because they are bounded by centromeric segments, and the fourth is a valid LST) Number of Telomeric Allelic Imbalances Allelic imbalance (AI) is the unequal contribution of parental allele sequences with or without changes in the overall copy number of the region. Our group have previously found, that the number telomeric AIs is indicative of defective DNA repair in ovarian cancer and triple-negative breast cancer, and that higher number of telomeric AI is associated with better response to cisplatin treatment (Birkbak et al. 2012). The number of telomeric allelic imbalances is the number AIs that extend to the telomeric end of a chromosome. Figure 3.A: Visual representation of the ntAI on short theoretical chromosomes. Figure 3.B: Illustration of possible telomeric allelic imbalances in an allele specific copy number profile. References https://github.com/sztup/scarHRD#running-on-mouse-genomes","title":"HRD Score"},{"location":"HRD_score/#hrd-score","text":"Task: Determine HRD Score Solution: scarHRD package ( https://github.com/sztup/scarHRD#running-on-mouse-genomes ) Description: scarHRD is an R package which determines the levels of homologous recombination deficiency (telomeric allelic imbalance, loss off heterozygosity, number of large-scale transitions) based on NGS (WES, WGS) data. The first genomic scar based homologous recombination deficiency measures were produced using SNP arrays. Since this technology has been largely replaced by next generation sequencing it has become important to develop algorithms that derive the same type of genomic scar-scores from next generation sequencing (WXS, WGS) data. In order to perform this analysis, here we introduce the scarHRD R package and show that using this method the SNP-array based and next generation sequencing based derivation of HRD scores show good correlation.","title":"HRD Score"},{"location":"HRD_score/#workflow-overview","text":"A typical workflow of determining the genomic scar scores for a tumor sample has the following steps: Call allele specific copy number profile on paired normal-tumor BAM files. This step has to be executed before running scarHRD. We recommend using Sequenza (Favero et al. 2015) http://www.cbs.dtu.dk/biotools/sequenza/ for copy number segmentation, Other tools (e.g. ASCAT (Van Loo et al. 2010)) may also be used in this step. This step is time-consuming and compute-intensive. Determine the scar scores with scarHRD R package. This step only takes a few minutes.","title":"Workflow overview"},{"location":"HRD_score/#sequenza","text":"","title":"Sequenza"},{"location":"HRD_score/#preprocessing-of-input-files","text":"In order to obtain precise mutational and aberration patterns in a tumor sample, Sequenza requires a matched normal sample from the same patient. Typically, the following files are needed to get started with Sequenza: A BAM file (or a derived pileup file) from the tumor specimen. A BAM file (or a derived pileup file) from the normal specimen. A FASTA reference genomic sequence file The normal and tumor BAM files are processed together to generate a seqz file, which is the required input for the analysis. It is possible to generate a seqz starting from other processed data, such as pileup, or VCF files. The available options are described in the sequenza-utils manual pages. The sequenza-utils command provides various tools; here we highlight only the basic usage: Process a FASTA file to produce a GC Wiggle track file (has to be done only once): sequenza\u2212utils gc_wiggle \u2212w 50 --fasta hg19.fa -o hg19.gc50Base.wig.gz Process BAM and Wiggle files to produce a seqz file: sequenza\u2212utils bam2seqz -n normal.bam -t tumor.bam --fasta hg19.fa \\ -gc hg19.gc50Base.wig.gz -o out.seqz.gz Post-process by binning the original seqz file: sequenza\u2212utils seqz_binning --seqz out.seqz.gz -w 50 -o out small.seqz.gz The small.seqz.gz would be the input to scarHRD .","title":"Preprocessing of input files"},{"location":"HRD_score/#scarhrd","text":"The scarHRD input may be a detailed segmentation file from Sequenza, in case there is a reliable estimation of ploidy of the tumor sample is known, it should be sumbitted in the ploidy argument of the scarHRD function, otherwise ploidy between 1 and 5.5 will be tested: a<-read.table(\"/examples/test1.small.seqz.gz\", header=T) head(a) ## chromosome position base.ref depth.normal depth.tumor depth.ratio Af ## 1 chr1 12975 N 7 20 2.841 1.000 ## 2 chr1 13020 A 8 28 3.500 0.964 ## 3 chr1 13026 N 15 43 2.964 1.000 ## 4 chr1 13038 T 11 35 3.182 0.971 ## 5 chr1 13041 A 11 37 3.364 0.946 ## 6 chr1 13077 N 26 65 2.465 1.000 ## Bf zygosity.normal GC.percent good.reads AB.normal AB.tumor tumor.strand ## 1 0 hom 60 51 N . 0 ## 2 0 hom 60 28 A G0.036 G1.0 ## 3 0 hom 59 51 N . 0 ## 4 0 hom 59 35 T C0.029 C1.0 ## 5 0 hom 59 37 A G0.054 G0.5 ## 6 0 hom 62 51 N . 0 Usage example: library(\"scarHRD\") scar_score(\"F:/Documents/scarHRD/examples/test1.small.seqz.gz\",reference = \"grch38\", seqz=TRUE) ## Preprocessing started... ## Processing chr1: 18 variant calls; 6290 heterozygous positions; 549112 homozygous positions. ## Processing chr2: 22 variant calls; 4934 heterozygous positions; 394216 homozygous positions. ## |=================================================================| 100% ## Preprocessing finished ## Determining HRD-LOH, LST, TAI ## HRD Telomeric AI LST HRD-sum ## [1,] 1 2 0 3 scar_score(\"F:/Documents/scarHRD/examples/test2.txt\",reference = \"grch38\", seqz=FALSE) ## Determining HRD-LOH, LST, TAI ## HRD Telomeric AI LST HRD-sum ## [1,] 25 35 33 93 Parameters seg -- input file name reference -- the reference genome used, grch38 or grch37 or mouse (default: grch38 ) seqz -- TRUE if the input file is a smallo.seqz.gz file, otherwise FALSE (default: TRUE ) ploidy -- optional, previously estimated ploidy of the sample outputdir -- optional, the path to the output directory chr.in.names -- optional, default: TRUE, set to FALSE if input file does not contain 'chr' in chromosome names.","title":"scarHRD"},{"location":"HRD_score/#genomic-scar-scores","text":"","title":"Genomic scar scores"},{"location":"HRD_score/#loss-of-heterozygosity-hrd-loh","text":"The HRD-LOH score was described based on investigation in SNP-array-based copy number profiles of ovarian cancer (Abkevich et al. 2012). In this paper the authors showed that the samples with deficient BRCA1, BRCA2 have higher HRD-LOH scores compared to BRCA-intact samples, thus this measurement may be a reliable tool to estimate the sample's homologous recombination capacity. The definition of a sample's HRD-LOH score is the number of 15 Mb exceeding LOH regions which do not cover the whole chromosome. In the first paper publishing HRD-LOH-score (Abkevich et al., 2012) the authors examine the correlation between HRD-LOH-score and HR deficiency calculated for different LOH region length cut-offs. In that paper the cut-off of 15 Mb approximately in the middle of the interval was arbitrarily selected for further analysis. The authors argue that the rational for this selection rather than selecting the cut-off with the lowest p-value is that the latter cut-off is more sensitive to statistical noise present in the data. In our manuscript we also investigated if this 15 Mb cutoff is appropriate for WXS-based HRD-LOH score.We followed the same principles as Abkievits et al, thus while there was small difference between the p-values for the different minimum length cutoff values, we chose to use the same, 15 Mb limit as Abkevich et al. We also performed Spearman rank correlation between the SNP-array-based and WXS-based HRD-LOH scores for the different cutoff minimum LOH length cutoff (manuscript, Supplementary Figure S3C). Here the 14 Mb and 15 Mb cutoff-based WXS-HRD-LOH score had the highest correlation with the SNP-based HRD score. (0.700 and 0.695 respectively). This result reassured our choice of using the 15 Mb cutoff like in the SNP-array-based HRD-LOH score. Figure 1.A Visual representation of the HRD-LOH score on short theoretical chromosomes. Figure 1.B: Calculating HRD-LOH from a biallelic copy-number profile; LOH regions a, and c, would both increase the score by 1, while neither b, or d, would add to its value (b, does not pass the length requirement, and d covers a whole chromosome) Large Scale Transitions (LST) The presence of Large Scale Transitions in connection with homologous recombination deficiency was first studied in basal-like breast cancer (Popova et al. 2012). Based on SNP-array derived copy number profiles BRCA1-inactivated cases had showed higher number of large scale transitions. A large scale transition is defined as a chromosomal break between adjacent regions of at least 10 Mb, with a distance between them not larger than 3Mb. Figure 2.A: Visual representation of the LST score on short theoretical chromosomes. Figure 2.B: Calculating LST scores from a biallelic copy-number profile; events that are marked with green \"marked\" signs would increase the score, while events marked with red crosses would not. The grey areas represent the centromeric regions. (From left to right; Chromosome 1: the first event passes the definition of an LST, the second bounded by a shorter than 10 Mb segment from the right, the third is bounded by a segment from the left, which extends to the centromere, the fourth\u2019s gap is greater than 3 Mb. Chromosome 2: The first event is a valid LST, the second and third are not because they are bounded by centromeric segments, and the fourth is a valid LST)","title":"Loss of Heterozygosity (HRD-LOH)"},{"location":"HRD_score/#number-of-telomeric-allelic-imbalances","text":"Allelic imbalance (AI) is the unequal contribution of parental allele sequences with or without changes in the overall copy number of the region. Our group have previously found, that the number telomeric AIs is indicative of defective DNA repair in ovarian cancer and triple-negative breast cancer, and that higher number of telomeric AI is associated with better response to cisplatin treatment (Birkbak et al. 2012). The number of telomeric allelic imbalances is the number AIs that extend to the telomeric end of a chromosome. Figure 3.A: Visual representation of the ntAI on short theoretical chromosomes. Figure 3.B: Illustration of possible telomeric allelic imbalances in an allele specific copy number profile.","title":"Number of Telomeric Allelic Imbalances"},{"location":"HRD_score/#references","text":"https://github.com/sztup/scarHRD#running-on-mouse-genomes","title":"References"},{"location":"Yaml_files/","text":"YAML (YANL Ain't Markup Language) Why is YAML popular? configuration files all writen in YAML (Docker, Ansible, Kubernetes) widely used format human readable and intuitive In general YAML is a data serialization language ( standard format to transfer data) like XML and JSON. YAML microservices: - app: user-authentication port: 9000 version: 1.0 XML <microservices> <microservice> <app>user-authentication</app> <port>9000</port> <version>1.0</version> <microservice> <microservices> JSON { \"microservices\": [ { \"app\": \"user-authentication\", \"port\": 9000, \"version\": \"1.0\" } ] } YMAL does not have special characters like <>, {}, [] you use line separation and indentation. Syntax Key-Value Pairs microservice: # comment here app: user-authentication port: 9000 # comment here version: 1.7 # boolean values deployed: yes deployed: true deployed: on # list list: - shopping - cart","title":"Yaml"},{"location":"Yaml_files/#yaml-yanl-aint-markup-language","text":"Why is YAML popular? configuration files all writen in YAML (Docker, Ansible, Kubernetes) widely used format human readable and intuitive In general YAML is a data serialization language ( standard format to transfer data) like XML and JSON. YAML microservices: - app: user-authentication port: 9000 version: 1.0 XML <microservices> <microservice> <app>user-authentication</app> <port>9000</port> <version>1.0</version> <microservice> <microservices> JSON { \"microservices\": [ { \"app\": \"user-authentication\", \"port\": 9000, \"version\": \"1.0\" } ] } YMAL does not have special characters like <>, {}, [] you use line separation and indentation.","title":"YAML (YANL Ain't Markup Language)"},{"location":"Yaml_files/#syntax","text":"","title":"Syntax"},{"location":"Yaml_files/#key-value-pairs","text":"microservice: # comment here app: user-authentication port: 9000 # comment here version: 1.7 # boolean values deployed: yes deployed: true deployed: on # list list: - shopping - cart","title":"Key-Value Pairs"},{"location":"about/","text":"","title":"About"},{"location":"git/","text":"GIT Benefits of Version Control Version control is a system that records changes to a file or set of files over time so that you can recall specific versions later. Track incremental backups and recover: Every document can be backed up automatically and restored at a second's notice. Track every change: Every infinitesimal change can be recorded and can be used to revert a file to an earlier state. Track writing experiments: Writing experiments can be sandboxed to copies while keeping the main file intact. Track co-authoring and collaboration: Teams can work independendtly on their own files, nut merge them into a latest common revision. Track individual contributions: Good VCS system tag changes with authors who make them. Git at a glance git commands The git tool has many subcommands that can be invoked like git subcommand for instance git status to get the status of a repository. The most important ones are: init : initialize a repository clone : clone a repository status : get information about a repository log : view the history and commit messages of the repository add : add a file to the staging area commit : commit your changes to your local repository push : push changes to a remote repository pull : pull changes from a remote repository checkout : retrieve a specific version of a file you can read more about each command by invoking the help: git commit --help git help commit Git concepts commit A commit is a recorded set of changes in your projects file's. Try to group logical sets of changes together into one commit - don't mix changes which are unreleated. repository A repository is the history of all your project's commits. Git settings Setting your identity Before we start, we should set the user name and e-email address. This is important because every git commit uses this information and it's also incredibly useful when looking at the history and commit log: git config --global user.name \"Sebastian Heucke\" git config --global user.email \"sebastian.heucke@med.uni-muenchen.de\" git config --global core.editor nano git config --global color.ui auto git config --global merge.tool kdiff3 A typical git workflow Creating Snapshots Initializing a repository git init Staging files git add file1.js # Stages a single file git add file1.js file2.js # Stages multiple files git add \\*.js # Stages with a pattern git add . # Stages the current directory and all its content Viewing the status git status # Full status git status -s # Short status Committing the staged files git commit -m \u201cMessage\u201d # Commits with a one-line message git commit # Opens the default editor to type a long message Skipping the staging area git commit -am \u201cMessage\u201d Removing files git rm file1.js # Removes from working directory and staging area git rm --cached file1.js # Removes from staging area only Renaming or moving files git mv file1.js file1.txt Viewing the staged/unstaged changes git diff # Shows unstaged changes git diff --staged # Shows staged changes git diff --cached # Same as the above Viewing the history git log # Full history git log --oneline # Summary git log --reverse # Lists the commits from the oldest to the newest Viewing a commit git show 921a2ff # Shows the given commit git show HEAD # Shows the last commit git show HEAD~2 # Two steps before the last commit git show HEAD:file.js # Shows the version of file.js stored in the last commit Unstaging files (undoing git add) git restore --staged file.js # Copies the last version of file.js from repo to index Discarding local changes git restore file.js # Copies file.js from index to working directory git restore file1.js file2.js # Restores multiple files in working directory git restore . # Discards all local changes (except untracked files) git clean -fd # Removes all untracked files Restoring an earlier version of a file git restore --source=HEAD~2 file.js Browsing History git log --stat # Shows the list of modified files git log --patch # Shows the actual changes (patches) Filtering the history git log -3 # Shows the last 3 entries git log --author=\u201cMosh\u201d git log --before=\u201c2020-08-17\u201d git log --after=\u201cone week ago\u201d git log --grep=\u201cGUI\u201d # Commits with \u201cGUI\u201d in their message git log -S\u201cGUI\u201d # Commits with \u201cGUI\u201d in their patches git log hash1..hash2 # Range of commits git log file.txt # Commits that touched file.txt Formatting the log output git log --pretty=format:\u201d%an committed %H\u201d Creating an alias git config --global alias.lg \u201clog --oneline\" View a commit git show HEAD~2 git show HEAD~2:file1.txt # Shows the version of file stored in this commit Comparing commits git diff HEAD~2 HEAD # Shows the changes between two commits git diff HEAD~2 HEAD file.txt # Changes to file.txt only Checking out a commit git checkout dad47ed # Checks out the given commit git checkout master # Checks out the master branch Finding a bad commit git bisect start git bisect bad # Marks the current commit as a bad commit git bisect good ca49180 # Marks the given commit as a good commit git bisect reset # Terminates the bisect session Finding contributors git shortlog Viewing the history of a file git log file.txt # Shows the commits that touched file.txt git log --stat file.txt # Shows statistics (the number of changes) for file.txt git log --patch file.txt # Shows the patches (changes) applied to file.txt Finding the author of lines git blame file.txt # Shows the author of each line in file.txt Tagging git tag v1.0 # Tags the last commit as v1.0 git tag v1.0 5e7a828 # Tags an earlier commit git tag # Lists all the tags git tag -d v1.0 # Deletes the given tag Branching & Merging Managing branches git branch bugfix # Creates a new branch called bugfix git checkout bugfix # Switches to the bugfix branch git switch bugfix # Same as the above git switch -C bugfix # Creates and switches git branch -d bugfix # Deletes the bugfix branch Comparing branches git log master..bugfix # Lists the commits in the bugfix branch not in master git diff master..bugfix # Shows the summary of changes Stashing git stash push -m \u201cNew tax rules\u201d # Creates a new stash git stash list # Lists all the stashes git stash show stash@{1} # Shows the given stash git stash show 1 # shortcut for stash@{1} git stash apply 1 # Applies the given stash to the working dir git stash drop 1 # Deletes the given stash git stash clear # Deletes all the stashes Merging git merge bugfix # Merges the bugfix branch into the current branch git merge --no-ff bugfix # Creates a merge commit even if FF is possible git merge --squash bugfix # Performs a squash merge git merge --abort # Aborts the merge Viewing the merged branches git branch --merged # Shows the merged branches git branch --no-merged # Shows the unmerged branches Rebasing git rebase master # Changes the base of the current branch Cherry picking git cherry-pick dad47ed # Applies the given commit on the current branch Collaboration Cloning a repository git clone url Syncing with remotes git fetch origin master # Fetches master from origin git fetch origin # Fetches all objects from origin git fetch # Shortcut for \u201cgit fetch origin\u201d git pull # Fetch + merge git push origin master # Pushes master to origin git push # Shortcut for \u201cgit push origin master\u201d Sharing tags git push origin v1.0 # Pushes tag v1.0 to origin git push origin \u2014delete v1.0 Sharing branches git branch -r # Shows remote tracking branches git branch -vv # Shows local & remote tracking branches git push -u origin bugfix # Pushes bugfix to origin git push -d origin bugfix # Removes bugfix from origin Managing remotes git remote # Shows remote repos git remote add upstream url # Adds a new remote called upstream git remote rm upstream # Remotes upstream Rewriting History Undoing commits git reset --soft HEAD^ # Removes the last commit, keeps changed staged git reset --mixed HEAD^ # Unstages the changes as well git reset --hard HEAD^ # Discards local changes Reverting commits git revert 72856ea # Reverts the given commit git revert HEAD~3.. # Reverts the last three commits git revert --no-commit HEAD~3.. Recovering lost commits git reflog # Shows the history of HEAD git reflog show bugfix # Shows the history of bugfix pointer Amending the last commit git commit --amend Interactive rebasing git rebase -i HEAD~5 .gitignore example https://www.gitignore.io/ # Byte-compiled / optimized / DLL files __pycache__/ *.py[cod] *$py.class # C extensions *.so # Distribution / packaging .Python build/ develop-eggs/ dist/ downloads/ eggs/ .eggs/ lib/ lib64/ parts/ sdist/ var/ wheels/ share/python-wheels/ *.egg-info/ .installed.cfg *.egg MANIFEST # PyInstaller # Usually these files are written by a python script from a template # before PyInstaller builds the exe, so as to inject date/other infos into it. *.manifest *.spec # Installer logs pip-log.txt pip-delete-this-directory.txt # Unit test / coverage reports htmlcov/ .tox/ .nox/ .coverage .coverage.* .cache nosetests.xml coverage.xml *.cover *.py,cover .hypothesis/ .pytest_cache/ cover/ # Translations *.mo *.pot # Django stuff: *.log local_settings.py db.sqlite3 db.sqlite3-journal # Flask stuff: instance/ .webassets-cache # Scrapy stuff: .scrapy # Sphinx documentation docs/_build/ # PyBuilder .pybuilder/ target/ # Jupyter Notebook .ipynb_checkpoints # IPython profile_default/ ipython_config.py # pyenv # For a library or package, you might want to ignore these files since the code is # intended to run in multiple environments; otherwise, check them in: # .python-version # pipenv # According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control. # However, in case of collaboration, if having platform-specific dependencies or dependencies # having no cross-platform support, pipenv may install dependencies that don't work, or not # install all needed dependencies. #Pipfile.lock # PEP 582; used by e.g. github.com/David-OConnor/pyflow __pypackages__/ # Celery stuff celerybeat-schedule celerybeat.pid # SageMath parsed files *.sage.py # Environments .env .venv env/ venv/ ENV/ env.bak/ venv.bak/ # Spyder project settings .spyderproject .spyproject # Rope project settings .ropeproject # mkdocs documentation /site # mypy .mypy_cache/ .dmypy.json dmypy.json # Pyre type checker .pyre/ # pytype static type analyzer .pytype/ # Cython debug symbols cython_debug/","title":"Git"},{"location":"git/#git","text":"","title":"GIT"},{"location":"git/#benefits-of-version-control","text":"Version control is a system that records changes to a file or set of files over time so that you can recall specific versions later. Track incremental backups and recover: Every document can be backed up automatically and restored at a second's notice. Track every change: Every infinitesimal change can be recorded and can be used to revert a file to an earlier state. Track writing experiments: Writing experiments can be sandboxed to copies while keeping the main file intact. Track co-authoring and collaboration: Teams can work independendtly on their own files, nut merge them into a latest common revision. Track individual contributions: Good VCS system tag changes with authors who make them.","title":"Benefits of Version Control"},{"location":"git/#git-at-a-glance","text":"","title":"Git at a glance"},{"location":"git/#git-commands","text":"The git tool has many subcommands that can be invoked like git subcommand for instance git status to get the status of a repository. The most important ones are: init : initialize a repository clone : clone a repository status : get information about a repository log : view the history and commit messages of the repository add : add a file to the staging area commit : commit your changes to your local repository push : push changes to a remote repository pull : pull changes from a remote repository checkout : retrieve a specific version of a file you can read more about each command by invoking the help: git commit --help git help commit","title":"git commands"},{"location":"git/#git-concepts","text":"","title":"Git concepts"},{"location":"git/#commit","text":"A commit is a recorded set of changes in your projects file's. Try to group logical sets of changes together into one commit - don't mix changes which are unreleated.","title":"commit"},{"location":"git/#repository","text":"A repository is the history of all your project's commits.","title":"repository"},{"location":"git/#git-settings","text":"","title":"Git settings"},{"location":"git/#setting-your-identity","text":"Before we start, we should set the user name and e-email address. This is important because every git commit uses this information and it's also incredibly useful when looking at the history and commit log: git config --global user.name \"Sebastian Heucke\" git config --global user.email \"sebastian.heucke@med.uni-muenchen.de\" git config --global core.editor nano git config --global color.ui auto git config --global merge.tool kdiff3","title":"Setting your identity"},{"location":"git/#a-typical-git-workflow","text":"","title":"A typical git workflow"},{"location":"git/#creating-snapshots","text":"","title":"Creating Snapshots"},{"location":"git/#initializing-a-repository","text":"git init","title":"Initializing a repository"},{"location":"git/#staging-files","text":"git add file1.js # Stages a single file git add file1.js file2.js # Stages multiple files git add \\*.js # Stages with a pattern git add . # Stages the current directory and all its content","title":"Staging files"},{"location":"git/#viewing-the-status","text":"git status # Full status git status -s # Short status","title":"Viewing the status"},{"location":"git/#committing-the-staged-files","text":"git commit -m \u201cMessage\u201d # Commits with a one-line message git commit # Opens the default editor to type a long message","title":"Committing the staged files"},{"location":"git/#skipping-the-staging-area","text":"git commit -am \u201cMessage\u201d","title":"Skipping the staging area"},{"location":"git/#removing-files","text":"git rm file1.js # Removes from working directory and staging area git rm --cached file1.js # Removes from staging area only","title":"Removing files"},{"location":"git/#renaming-or-moving-files","text":"git mv file1.js file1.txt","title":"Renaming or moving files"},{"location":"git/#viewing-the-stagedunstaged-changes","text":"git diff # Shows unstaged changes git diff --staged # Shows staged changes git diff --cached # Same as the above","title":"Viewing the staged/unstaged changes"},{"location":"git/#viewing-the-history","text":"git log # Full history git log --oneline # Summary git log --reverse # Lists the commits from the oldest to the newest","title":"Viewing the history"},{"location":"git/#viewing-a-commit","text":"git show 921a2ff # Shows the given commit git show HEAD # Shows the last commit git show HEAD~2 # Two steps before the last commit git show HEAD:file.js # Shows the version of file.js stored in the last commit","title":"Viewing a commit"},{"location":"git/#unstaging-files-undoing-git-add","text":"git restore --staged file.js # Copies the last version of file.js from repo to index","title":"Unstaging files (undoing git add)"},{"location":"git/#discarding-local-changes","text":"git restore file.js # Copies file.js from index to working directory git restore file1.js file2.js # Restores multiple files in working directory git restore . # Discards all local changes (except untracked files) git clean -fd # Removes all untracked files","title":"Discarding local changes"},{"location":"git/#restoring-an-earlier-version-of-a-file","text":"git restore --source=HEAD~2 file.js","title":"Restoring an earlier version of a file"},{"location":"git/#browsing-history","text":"git log --stat # Shows the list of modified files git log --patch # Shows the actual changes (patches)","title":"Browsing History"},{"location":"git/#filtering-the-history","text":"git log -3 # Shows the last 3 entries git log --author=\u201cMosh\u201d git log --before=\u201c2020-08-17\u201d git log --after=\u201cone week ago\u201d git log --grep=\u201cGUI\u201d # Commits with \u201cGUI\u201d in their message git log -S\u201cGUI\u201d # Commits with \u201cGUI\u201d in their patches git log hash1..hash2 # Range of commits git log file.txt # Commits that touched file.txt","title":"Filtering the history"},{"location":"git/#formatting-the-log-output","text":"git log --pretty=format:\u201d%an committed %H\u201d","title":"Formatting the log output"},{"location":"git/#creating-an-alias","text":"git config --global alias.lg \u201clog --oneline\"","title":"Creating an alias"},{"location":"git/#view-a-commit","text":"git show HEAD~2 git show HEAD~2:file1.txt # Shows the version of file stored in this commit","title":"View a commit"},{"location":"git/#comparing-commits","text":"git diff HEAD~2 HEAD # Shows the changes between two commits git diff HEAD~2 HEAD file.txt # Changes to file.txt only","title":"Comparing commits"},{"location":"git/#checking-out-a-commit","text":"git checkout dad47ed # Checks out the given commit git checkout master # Checks out the master branch","title":"Checking out a commit"},{"location":"git/#finding-a-bad-commit","text":"git bisect start git bisect bad # Marks the current commit as a bad commit git bisect good ca49180 # Marks the given commit as a good commit git bisect reset # Terminates the bisect session","title":"Finding a bad commit"},{"location":"git/#finding-contributors","text":"git shortlog","title":"Finding contributors"},{"location":"git/#viewing-the-history-of-a-file","text":"git log file.txt # Shows the commits that touched file.txt git log --stat file.txt # Shows statistics (the number of changes) for file.txt git log --patch file.txt # Shows the patches (changes) applied to file.txt","title":"Viewing the history of a file"},{"location":"git/#finding-the-author-of-lines","text":"git blame file.txt # Shows the author of each line in file.txt","title":"Finding the author of lines"},{"location":"git/#tagging","text":"git tag v1.0 # Tags the last commit as v1.0 git tag v1.0 5e7a828 # Tags an earlier commit git tag # Lists all the tags git tag -d v1.0 # Deletes the given tag","title":"Tagging"},{"location":"git/#branching-merging","text":"","title":"Branching &amp; Merging"},{"location":"git/#managing-branches","text":"git branch bugfix # Creates a new branch called bugfix git checkout bugfix # Switches to the bugfix branch git switch bugfix # Same as the above git switch -C bugfix # Creates and switches git branch -d bugfix # Deletes the bugfix branch","title":"Managing branches"},{"location":"git/#comparing-branches","text":"git log master..bugfix # Lists the commits in the bugfix branch not in master git diff master..bugfix # Shows the summary of changes","title":"Comparing branches"},{"location":"git/#stashing","text":"git stash push -m \u201cNew tax rules\u201d # Creates a new stash git stash list # Lists all the stashes git stash show stash@{1} # Shows the given stash git stash show 1 # shortcut for stash@{1} git stash apply 1 # Applies the given stash to the working dir git stash drop 1 # Deletes the given stash git stash clear # Deletes all the stashes","title":"Stashing"},{"location":"git/#merging","text":"git merge bugfix # Merges the bugfix branch into the current branch git merge --no-ff bugfix # Creates a merge commit even if FF is possible git merge --squash bugfix # Performs a squash merge git merge --abort # Aborts the merge","title":"Merging"},{"location":"git/#viewing-the-merged-branches","text":"git branch --merged # Shows the merged branches git branch --no-merged # Shows the unmerged branches","title":"Viewing the merged branches"},{"location":"git/#rebasing","text":"git rebase master # Changes the base of the current branch","title":"Rebasing"},{"location":"git/#cherry-picking","text":"git cherry-pick dad47ed # Applies the given commit on the current branch","title":"Cherry picking"},{"location":"git/#collaboration","text":"","title":"Collaboration"},{"location":"git/#cloning-a-repository","text":"git clone url","title":"Cloning a repository"},{"location":"git/#syncing-with-remotes","text":"git fetch origin master # Fetches master from origin git fetch origin # Fetches all objects from origin git fetch # Shortcut for \u201cgit fetch origin\u201d git pull # Fetch + merge git push origin master # Pushes master to origin git push # Shortcut for \u201cgit push origin master\u201d","title":"Syncing with remotes"},{"location":"git/#sharing-tags","text":"git push origin v1.0 # Pushes tag v1.0 to origin git push origin \u2014delete v1.0","title":"Sharing tags"},{"location":"git/#sharing-branches","text":"git branch -r # Shows remote tracking branches git branch -vv # Shows local & remote tracking branches git push -u origin bugfix # Pushes bugfix to origin git push -d origin bugfix # Removes bugfix from origin","title":"Sharing branches"},{"location":"git/#managing-remotes","text":"git remote # Shows remote repos git remote add upstream url # Adds a new remote called upstream git remote rm upstream # Remotes upstream","title":"Managing remotes"},{"location":"git/#rewriting-history","text":"","title":"Rewriting History"},{"location":"git/#undoing-commits","text":"git reset --soft HEAD^ # Removes the last commit, keeps changed staged git reset --mixed HEAD^ # Unstages the changes as well git reset --hard HEAD^ # Discards local changes","title":"Undoing commits"},{"location":"git/#reverting-commits","text":"git revert 72856ea # Reverts the given commit git revert HEAD~3.. # Reverts the last three commits git revert --no-commit HEAD~3..","title":"Reverting commits"},{"location":"git/#recovering-lost-commits","text":"git reflog # Shows the history of HEAD git reflog show bugfix # Shows the history of bugfix pointer","title":"Recovering lost commits"},{"location":"git/#amending-the-last-commit","text":"git commit --amend","title":"Amending the last commit"},{"location":"git/#interactive-rebasing","text":"git rebase -i HEAD~5","title":"Interactive rebasing"},{"location":"git/#gitignore-example","text":"https://www.gitignore.io/ # Byte-compiled / optimized / DLL files __pycache__/ *.py[cod] *$py.class # C extensions *.so # Distribution / packaging .Python build/ develop-eggs/ dist/ downloads/ eggs/ .eggs/ lib/ lib64/ parts/ sdist/ var/ wheels/ share/python-wheels/ *.egg-info/ .installed.cfg *.egg MANIFEST # PyInstaller # Usually these files are written by a python script from a template # before PyInstaller builds the exe, so as to inject date/other infos into it. *.manifest *.spec # Installer logs pip-log.txt pip-delete-this-directory.txt # Unit test / coverage reports htmlcov/ .tox/ .nox/ .coverage .coverage.* .cache nosetests.xml coverage.xml *.cover *.py,cover .hypothesis/ .pytest_cache/ cover/ # Translations *.mo *.pot # Django stuff: *.log local_settings.py db.sqlite3 db.sqlite3-journal # Flask stuff: instance/ .webassets-cache # Scrapy stuff: .scrapy # Sphinx documentation docs/_build/ # PyBuilder .pybuilder/ target/ # Jupyter Notebook .ipynb_checkpoints # IPython profile_default/ ipython_config.py # pyenv # For a library or package, you might want to ignore these files since the code is # intended to run in multiple environments; otherwise, check them in: # .python-version # pipenv # According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control. # However, in case of collaboration, if having platform-specific dependencies or dependencies # having no cross-platform support, pipenv may install dependencies that don't work, or not # install all needed dependencies. #Pipfile.lock # PEP 582; used by e.g. github.com/David-OConnor/pyflow __pypackages__/ # Celery stuff celerybeat-schedule celerybeat.pid # SageMath parsed files *.sage.py # Environments .env .venv env/ venv/ ENV/ env.bak/ venv.bak/ # Spyder project settings .spyderproject .spyproject # Rope project settings .ropeproject # mkdocs documentation /site # mypy .mypy_cache/ .dmypy.json dmypy.json # Pyre type checker .pyre/ # pytype static type analyzer .pytype/ # Cython debug symbols cython_debug/","title":".gitignore example"},{"location":"tasks/","text":"Initialization 0 Prerequisites You need to have an elixir account to become a member of our training project at the de.NBI cloud in Berlin Your public ssh must be added at the de.NBI portal: https://cloud.denbi.de/portal/ (or at the elixir portal) to able to connect to the jumphost 1 Deploy a vm in training project Login with elixir account at de.NBI cloud Berlin horizon dashboard (OpenStack graphical interface): https://denbi-cloud.bihealth.org/ First time users need to add their public ssh key to the horizon dashboard, to be able to inject your public key when you create a new vm: Project --> Compute --> Key Pairs --> Import Public Key Create a Theia virtual machine: Project --> Compute --> Instances --> Launch Instance Details: set an Instance Name with your zoomname in it Source: set Create new volume to no and choose \"theia\" image Flavor: de.NBI default Networks: select onlineTraining-network Key pair: choose YOURKEY Launch instance Assign a floating ip to get ssh access: _Project --> Compute --> Instances --> choose your instance --> assign floating vm Assign floating IP from range 172.16.102.x or172.16.103.x Please make sure you did not accidentally take a floating ip address from another ip range. They are reserved for another use case and will not work with you vm. 2 Connect to instance via ssh Connection to OpenStack vm at de.NBI cloud site Berlin is only possible via jumphost. Therefore you have two options: A) Manually jump from your client to jumphost and from there further to your vm with ssh-agent-forwarding Start ssh-agent: bash eval `ssh-agent` // eval $(ssh-agent) Add ssh private key: bash ssh-add .ssh/id_rsa show identities: bash ssh-add -l Connect at first from your client to jumphost: bash ssh -A yourElixirUserName@denbi-jumphost-01.bihealth.org And from the jumphost you can connect further to the floating ip of your vm bash ssh ubuntu@yourFloatingIpOfVM B) Setup ssh-config under .ssh/config Windows 10: Host denbi-jumphost-01.bihealth.org HostName denbi-jumphost-01.bihealth.org User yourElixirUserName IdentityFile PATH_TO_KEY Host training_snakemake HostName 172.16.XXX.XXX IdentityFile PATH_TO_KEY User ubuntu ProxyCommand C:\\Windows\\System32\\OpenSSH\\ssh.exe denbi-jumphost-01.bihealth.org -W %h:%p LocalForward 8181 localhost:8181 Linux: Host denbi-jumphost-01.bihealth.org HostName denbi-jumphost-01.bihealth.org User yourElixirUserName IdentityFile PATH_TO_KEY Host training_snakemake HostName 172.16.XXX.XXX IdentityFile PATH_TO_KEY User ubuntu ProxyJump denbi-jumphost-01.bihealth.org LocalForward 8181 localhost:8181 # other option is to use ssh -L 8181:localhost:8181 ubuntu@training_snakemake 3. Setup project Connect to instance via ssh and to Theia via browser (http://localhost:8181/#/home/ubuntu) Clone repository cd $HOME git clone https://gitlab.com/twardzso/3rd_denbi_user_meeting__snakemake_cloud.git Download and install conda (type: empty , yes , empty , yes ) wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh /bin/bash Miniconda3-latest-Linux-x86_64.sh source ~/.bashrc Install conda environment cd $HOME/3rd_denbi_user_meeting__snakemake_cloud/1_init/ conda env create --file environment.yaml conda activate snakemake_training","title":"Initialization"},{"location":"tasks/#initialization","text":"","title":"Initialization"},{"location":"tasks/#0-prerequisites","text":"You need to have an elixir account to become a member of our training project at the de.NBI cloud in Berlin Your public ssh must be added at the de.NBI portal: https://cloud.denbi.de/portal/ (or at the elixir portal) to able to connect to the jumphost","title":"0 Prerequisites"},{"location":"tasks/#1-deploy-a-vm-in-training-project","text":"Login with elixir account at de.NBI cloud Berlin horizon dashboard (OpenStack graphical interface): https://denbi-cloud.bihealth.org/ First time users need to add their public ssh key to the horizon dashboard, to be able to inject your public key when you create a new vm: Project --> Compute --> Key Pairs --> Import Public Key Create a Theia virtual machine: Project --> Compute --> Instances --> Launch Instance Details: set an Instance Name with your zoomname in it Source: set Create new volume to no and choose \"theia\" image Flavor: de.NBI default Networks: select onlineTraining-network Key pair: choose YOURKEY Launch instance Assign a floating ip to get ssh access: _Project --> Compute --> Instances --> choose your instance --> assign floating vm Assign floating IP from range 172.16.102.x or172.16.103.x Please make sure you did not accidentally take a floating ip address from another ip range. They are reserved for another use case and will not work with you vm.","title":"1 Deploy a vm in training project"},{"location":"tasks/#2-connect-to-instance-via-ssh","text":"Connection to OpenStack vm at de.NBI cloud site Berlin is only possible via jumphost. Therefore you have two options: A) Manually jump from your client to jumphost and from there further to your vm with ssh-agent-forwarding Start ssh-agent: bash eval `ssh-agent` // eval $(ssh-agent) Add ssh private key: bash ssh-add .ssh/id_rsa show identities: bash ssh-add -l Connect at first from your client to jumphost: bash ssh -A yourElixirUserName@denbi-jumphost-01.bihealth.org And from the jumphost you can connect further to the floating ip of your vm bash ssh ubuntu@yourFloatingIpOfVM B) Setup ssh-config under .ssh/config Windows 10: Host denbi-jumphost-01.bihealth.org HostName denbi-jumphost-01.bihealth.org User yourElixirUserName IdentityFile PATH_TO_KEY Host training_snakemake HostName 172.16.XXX.XXX IdentityFile PATH_TO_KEY User ubuntu ProxyCommand C:\\Windows\\System32\\OpenSSH\\ssh.exe denbi-jumphost-01.bihealth.org -W %h:%p LocalForward 8181 localhost:8181 Linux: Host denbi-jumphost-01.bihealth.org HostName denbi-jumphost-01.bihealth.org User yourElixirUserName IdentityFile PATH_TO_KEY Host training_snakemake HostName 172.16.XXX.XXX IdentityFile PATH_TO_KEY User ubuntu ProxyJump denbi-jumphost-01.bihealth.org LocalForward 8181 localhost:8181 # other option is to use ssh -L 8181:localhost:8181 ubuntu@training_snakemake","title":"2 Connect to instance via ssh"},{"location":"tasks/#3-setup-project","text":"Connect to instance via ssh and to Theia via browser (http://localhost:8181/#/home/ubuntu) Clone repository cd $HOME git clone https://gitlab.com/twardzso/3rd_denbi_user_meeting__snakemake_cloud.git Download and install conda (type: empty , yes , empty , yes ) wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh /bin/bash Miniconda3-latest-Linux-x86_64.sh source ~/.bashrc Install conda environment cd $HOME/3rd_denbi_user_meeting__snakemake_cloud/1_init/ conda env create --file environment.yaml conda activate snakemake_training","title":"3. Setup project"}]}